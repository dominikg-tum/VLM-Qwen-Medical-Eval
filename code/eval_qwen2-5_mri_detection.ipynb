{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bdf00a",
   "metadata": {},
   "source": [
    "# ðŸ§  Qwen2.5 Brain MRI Abnormality Detection\n",
    "\n",
    "Locate abnormal regions in MRI slices using Qwen2.5 and evaluate with mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041d2d4",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, base64\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def encode_image_to_data_uri(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "DATASET_DIR = \"VLM-Seminar25-Dataset/nova_brain\"\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, \"images\")\n",
    "ANNOT_PATH = os.path.join(DATASET_DIR, \"annotations.json\")\n",
    "RESULTS_DIR = \"../results/nova_brain\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "with open(ANNOT_PATH, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "case_ids = list(annotations.keys())\n",
    "\n",
    "load_dotenv(dotenv_path=\"config/user.env\")\n",
    "api_key = os.environ.get(\"NEBIUS_API_KEY\")\n",
    "client = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_new_inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f4682",
   "metadata": {},
   "source": [
    "## 2. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_results = []\n",
    "if do_new_inference:\n",
    "    for case_id in tqdm(case_ids):\n",
    "        case = annotations[case_id]\n",
    "        for img_name, img_info in case.get(\"image_findings\", {}).items():\n",
    "            img_path = os.path.join(IMAGES_DIR, img_name)\n",
    "            data_uri = encode_image_to_data_uri(img_path)\n",
    "            prompt = \"Please locate any abnormal areas in the MRI image and output the bounding boxes as [x1, y1, x2, y2].\"\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            pred = completion.choices[0].message.content.strip()\n",
    "            detection_results.append({\n",
    "                \"case_id\": case_id,\n",
    "                \"image\": img_name,\n",
    "                \"prediction\": pred\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c75633",
   "metadata": {},
   "source": [
    "## 3. Save Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8de433",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_DIR, \"qwen2.5_detection_results.json\"), \"w\") as f:\n",
    "    json.dump(detection_results, f, indent=2)\n",
    "print(\"Saved detection results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a28bd",
   "metadata": {},
   "source": [
    "Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e18f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_DIR, \"qwen2.5_detection_results.json\"), \"r\") as f:\n",
    "    detection_results = json.load(f)\n",
    "print(f\"Number of detections: {len(detection_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf635461",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"VLM-Seminar25-Dataset/scripts/calculate_map.py\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "\n",
    "m = re.search(r\"mAP\\s*[:=]\\s*([0-9.]+)\", result.stdout)\n",
    "if m:\n",
    "    mAP = float(m.group(1))\n",
    "    eval_metrics = {\"mAP\": mAP}\n",
    "    with open(os.path.join(RESULTS_DIR, \"detection_eval_metrics.json\"), \"w\") as f:\n",
    "        json.dump(eval_metrics, f, indent=2)\n",
    "    print(\"Saved mAP metric.\")\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.bar([\"mAP\"], [mAP], color=\"mediumseagreen\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Qwen2.5 MRI Detection mAP\")\n",
    "    plt.text(0, mAP + 0.02, f\"{mAP:.2f}\", ha='center', fontsize=12)\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"detection_metrics.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"Saved mAP plot.\")\n",
    "else:\n",
    "    print(\"Could not parse mAP from evaluation output.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
