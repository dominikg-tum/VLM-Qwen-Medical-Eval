{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a9239d",
   "metadata": {},
   "source": [
    "# ðŸ§  Qwen2.5 Brain MRI Description Generation\n",
    "\n",
    "Generate medical descriptions for each MRI slice using Qwen2.5 and evaluate with BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a112a1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, base64\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def encode_image_to_data_uri(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "DATASET_DIR = \"VLM-Seminar25-Dataset/nova_brain\"\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, \"images\")\n",
    "ANNOT_PATH = os.path.join(DATASET_DIR, \"annotations.json\")\n",
    "RESULTS_DIR = \"../results/nova_brain\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "with open(ANNOT_PATH, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "case_ids = list(annotations.keys())\n",
    "\n",
    "load_dotenv(dotenv_path=\"config/user.env\")\n",
    "api_key = os.environ.get(\"NEBIUS_API_KEY\")\n",
    "client = OpenAI(base_url=\"https://api.studio.nebius.com/v1/\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03935faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_new_inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14726208",
   "metadata": {},
   "source": [
    "## 2. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_results = []\n",
    "if do_new_inference:\n",
    "    for case_id in tqdm(case_ids):\n",
    "        case = annotations[case_id]\n",
    "        for img_name, img_info in case.get(\"image_findings\", {}).items():\n",
    "            img_path = os.path.join(IMAGES_DIR, img_name)\n",
    "            data_uri = encode_image_to_data_uri(img_path)\n",
    "            prompt = \"Please describe the given medical image.\"\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            pred = completion.choices[0].message.content.strip()\n",
    "            description_results.append({\n",
    "                \"case_id\": case_id,\n",
    "                \"image\": img_name,\n",
    "                \"prediction\": pred,\n",
    "                \"ground_truth\": img_info.get(\"caption\", \"\")\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9d6fc",
   "metadata": {},
   "source": [
    "## 3. Save Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_DIR, \"qwen2.5_description_results.json\"), \"w\") as f:\n",
    "    json.dump(description_results, f, indent=2)\n",
    "print(\"Saved description results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff7f46",
   "metadata": {},
   "source": [
    "Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73328dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_DIR, \"qwen2.5_description_results.json\"), \"r\") as f:\n",
    "    description_results = json.load(f)\n",
    "print(f\"Number of descriptions: {len(description_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da712ccc",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ff05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally import from /code/eval_scripts/ if available\n",
    "import sys\n",
    "sys.path.append(\"VLM-Seminar25-Dataset/scripts\")\n",
    "from evaluate_bleu import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "gt = [x[\"ground_truth\"] for x in description_results]\n",
    "pred = [x[\"prediction\"] for x in description_results]\n",
    "\n",
    "bleu_scores = []\n",
    "for ref, cand in zip(gt, pred):\n",
    "    ref_tokens = word_tokenize(ref)\n",
    "    cand_tokens = word_tokenize(cand)\n",
    "    bleu = sentence_bleu([ref_tokens], cand_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    bleu_scores.append(bleu)\n",
    "mean_bleu = sum(bleu_scores)/len(bleu_scores) if bleu_scores else 0.0\n",
    "print(f\"Mean BLEU-4: {mean_bleu:.4f}\")\n",
    "\n",
    "eval_metrics = {\"mean_bleu_4\": mean_bleu}\n",
    "with open(os.path.join(RESULTS_DIR, \"description_eval_metrics.json\"), \"w\") as f:\n",
    "    json.dump(eval_metrics, f, indent=2)\n",
    "print(\"Saved BLEU metrics.\")\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.bar([\"BLEU-4\"], [mean_bleu], color=\"orchid\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Qwen2.5 MRI Description BLEU-4\")\n",
    "plt.text(0, mean_bleu + 0.02, f\"{mean_bleu:.2f}\", ha='center', fontsize=12)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"description_metrics.png\"))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved BLEU-4 plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c58876",
   "metadata": {},
   "source": [
    "## 5. Visualize Results: Correct Descriptions (BLEU â‰¥ 0.5)\n",
    "Show examples where the BLEU score is high (â‰¥ 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a75db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def show_description_examples(examples, title, max_n=4):\n",
    "    n = min(len(examples), max_n)\n",
    "    if n == 0:\n",
    "        print(f\"No examples for {title}\")\n",
    "        return\n",
    "    fig, axes = plt.subplots(1, n, figsize=(6*n, 6))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, (img_path, gt, pred, bleu) in enumerate(examples[:n]):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"BLEU: {bleu:.2f}\", fontsize=14)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].text(0, -10, f\"GT: {gt}\\n\\nPred: {pred}\", fontsize=10, wrap=True)\n",
    "    plt.suptitle(title, fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "correct = []\n",
    "for x, bleu in zip(description_results, bleu_scores):\n",
    "    if bleu >= 0.5:\n",
    "        img_path = os.path.join(IMAGES_DIR, x[\"image\"])\n",
    "        correct.append((img_path, x[\"ground_truth\"], x[\"prediction\"], bleu))\n",
    "\n",
    "show_description_examples(correct, \"Correct Descriptions (BLEU â‰¥ 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d5ddb",
   "metadata": {},
   "source": [
    "## 6. Visualize Results: Incorrect Descriptions (BLEU < 0.5)\n",
    "Show examples where the BLEU score is low (&lt; 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = []\n",
    "for x, bleu in zip(description_results, bleu_scores):\n",
    "    if bleu < 0.5:\n",
    "        img_path = os.path.join(IMAGES_DIR, x[\"image\"])\n",
    "        incorrect.append((img_path, x[\"ground_truth\"], x[\"prediction\"], bleu))\n",
    "\n",
    "show_description_examples(incorrect, \"Incorrect Descriptions (BLEU < 0.5)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
