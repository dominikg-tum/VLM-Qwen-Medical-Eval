{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee572995",
   "metadata": {},
   "source": [
    "# ü´Å Qwen2.5 Chest X-ray Classification\n",
    "\n",
    "Classify each chest X-ray as 'healthy' or 'unhealthy' using Qwen2.5 and evaluate with accuracy/F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, base64\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encode_image_to_data_uri(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "DATASET_DIR = \"VLM-Seminar25-Dataset/chest_xrays\"\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, \"images\")\n",
    "ANNOT_PATH = os.path.join(DATASET_DIR, \"annotations_len_50.json\")\n",
    "\n",
    "with open(ANNOT_PATH, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "image_ids = list(annotations.keys())\n",
    "\n",
    "load_dotenv(dotenv_path=\"../config/user.env\")\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 12/50 [01:17<05:35,  8.84s/it]"
     ]
    }
   ],
   "source": [
    "classification_results = []\n",
    "for img_id in tqdm(image_ids):\n",
    "    img_path = os.path.join(IMAGES_DIR, img_id + \".png\")\n",
    "    data_uri = encode_image_to_data_uri(img_path)\n",
    "    prompt = \"Given the medical image, classify it as 'healthy' or 'unhealthy'. It is very important that you only output only either 'healthy' or 'unhealthy'.\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen/qwen2.5-vl-72b-instruct:free\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    pred = completion.choices[0].message.content.strip().lower()\n",
    "    classification_results.append({\"id\": img_id, \"prediction\": pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "with open(\"VLM-Seminar25-Dataset/chest_xrays/qwen2_5_classification_results.json\", \"w\") as f:\n",
    "    json.dump(classification_results, f, indent=2)\n",
    "print(\"Saved classification results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a368939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation using direct import from provided script\n",
    "import sys\n",
    "sys.path.append(\"VLM-Seminar25-Dataset/scripts\")\n",
    "from evaluate_metrics import accuracy_score, f1_score\n",
    "\n",
    "gt = [annotations[x[\"id\"]][\"status\"] for x in classification_results]\n",
    "pred = [x[\"prediction\"] for x in classification_results]\n",
    "\n",
    "accuracy = accuracy_score(gt, pred)\n",
    "f1 = f1_score(gt, pred, pos_label='unhealthy')\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dc42c",
   "metadata": {},
   "source": [
    "## Visualize Results: Correct Predictions\n",
    "\n",
    "Show images where the prediction matches the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18122d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Visualization config (from instruction.ipynb)\n",
    "FONT_PATH = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "FONT_SIZE = 32\n",
    "\n",
    "def draw_text(draw, pos, text, font, color, outline=2):\n",
    "    for dx in [-outline, outline]:\n",
    "        for dy in [-outline, outline]:\n",
    "            draw.text((pos[0]+dx, pos[1]+dy), text, font=font, fill='black')\n",
    "    draw.text(pos, text, font=font, fill=color)\n",
    "\n",
    "def visualize_img(img_id, gt_label, pred_label):\n",
    "    img_path = os.path.join(IMAGES_DIR, img_id + \".png\")\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    try:\n",
    "        font = ImageFont.truetype(FONT_PATH, FONT_SIZE)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    text = f\"GT: {gt_label}\\nPred: {pred_label}\"\n",
    "    draw_text(draw, (10, 10), text, font, color=\"green\" if gt_label == pred_label else \"red\")\n",
    "    return img\n",
    "\n",
    "# Split into correct and incorrect\n",
    "correct = []\n",
    "incorrect = []\n",
    "for x in classification_results:\n",
    "    gt_label = annotations[x[\"id\"]][\"status\"]\n",
    "    pred_label = x[\"prediction\"]\n",
    "    if gt_label == pred_label:\n",
    "        correct.append((x[\"id\"], gt_label, pred_label))\n",
    "    else:\n",
    "        incorrect.append((x[\"id\"], gt_label, pred_label))\n",
    "\n",
    "def show_examples(examples, title, max_n=6):\n",
    "    n = min(len(examples), max_n)\n",
    "    if n == 0:\n",
    "        print(f\"No examples for {title}\")\n",
    "        return\n",
    "    fig, axes = plt.subplots(1, n, figsize=(5*n, 5))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, (img_id, gt_label, pred_label) in enumerate(examples[:n]):\n",
    "        img = visualize_img(img_id, gt_label, pred_label)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"ID: {img_id}\", fontsize=14)\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(title, fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_examples(correct, \"Correct Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ddd06",
   "metadata": {},
   "source": [
    "## Visualize Results: Incorrect Predictions\n",
    "\n",
    "Show images where the prediction does not match the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_examples(incorrect, \"Incorrect Predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
